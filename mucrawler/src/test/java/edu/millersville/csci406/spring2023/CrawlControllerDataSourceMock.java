package edu.millersville.csci406.spring2023;

import java.net.URL;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;
import java.util.Map;
import java.util.Set;

import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertTrue;

/**
 * A DataSource implementation for testing the CrawlController class.
 * 
 * @author Chad Hogg
 * @version 2023-01-19
 */
public class CrawlControllerDataSourceMock extends AbstractCrawlingDataSourceMock {
	
	/** A set of CrawlJobs that the CrawlController should complete. */
	private Set<CrawlJob> outstandingJobs;
	/** A mapping from each CrawlJob to the Set of RobotsRules that we expect to receive for it.  (No entry for non-robots.txt jobs.) */
	private Map<CrawlJob, Set<RobotsRule>> expectedRules;
	/** A mapping from each CrawlJob to the Set of RobotsRules that we actually did receive for it. (No entry if we did not receive a Set of RobotsRules for it.) */
	private Map<CrawlJob, Set<RobotsRule>> receivedRules;
	/** A mapping from each CrawlJob to the Set of CrawlJobs that should be disallowed because of it.  (No entry for non-robots.txt jobs.) */
	private Map<CrawlJob, Set<CrawlJob>> disallowedJobs;
	/** A mapping from each CrawlJob to the Set of URLs that we expect to receive for it.  (No entry for robots.txt jobs.) */
	private Map<CrawlJob, Set<URL>> expectedUrls;
	/** A mapping from each CrawlJob to the Set of URLs that we actually did receive for it.  (No entry if we did not receive a Set of URLs for it. */
	private Map<CrawlJob, Set<URL>> receivedUrls;
	/** A mapping from each CrawlJob to the String of content that we expect to receive for it.  (No entry for robots.txt jobs.) */
	private Map<CrawlJob, String> expectedContent;
	/** A mapping from each CrawlJob to the String of content that we actually did receive for it.  (No entry if we did not receive a String of content for it. */
	private Map<CrawlJob, String> receivedContent;
	/** A mapping from each CrawlJob to the Set of new CrawlJobs generated by it. (No entry for robots.txt jobs.) */
	private Map<CrawlJob, Set<CrawlJob>> newJobs;
	/** A set of CrawlJobs that should be canceled. */
	private Set<CrawlJob> expectedCancellations;
	/** A set of CrawlJobs that were canceled. */
	private Set<CrawlJob> receivedCancellations;
	
	/**
	 * Constructs a CrawlControllerDataSourceMock from all of the information that it should provide to the CrawlController and expect to receive from the CrawlController.
	 * 
	 * @param initialJobs A Set of CrawlJobs that the CrawlController should completed.
	 * @param expectedRules A mapping from CrawlJob to Set of RobotsRules that should be received for it.
	 * @param disallowedJobs A mapping from CrawlJob to Set of CrawlJobs that should be disallowed by it.
	 * @param expectedURLs A mapping from CrawlJob to Set of URLs that should be received for it.
	 * @param expectedContent A mapping from CrawlJob to String of content that should be received for it.
	 * @param newJobs A mapping from CrawlJob to Set of CrawlJobs generated from it.
	 * @param expectedCancellations A set of CrawlJobs that should be canceled.
	 */
	public CrawlControllerDataSourceMock(Set<CrawlJob> initialJobs, Map<CrawlJob, Set<RobotsRule>> expectedRules, Map<CrawlJob, Set<CrawlJob>> disallowedJobs, Map<CrawlJob, Set<URL>> expectedURLs, Map<CrawlJob, String> expectedContent, Map<CrawlJob, Set<CrawlJob>> newJobs, Set<CrawlJob> expectedCancellations) {
		this.outstandingJobs = initialJobs;
		this.expectedRules = expectedRules;
		this.receivedRules = new HashMap<>();
		this.disallowedJobs = disallowedJobs;
		this.expectedUrls = expectedURLs;
		this.receivedUrls = new HashMap<>();
		this.expectedContent = expectedContent;
		this.receivedContent = new HashMap<>();
		this.newJobs = newJobs;
		this.expectedCancellations = expectedCancellations;
		this.receivedCancellations = new HashSet<>();
	}

	@Override
	public Set<CrawlJob> getURLsToCrawl() throws DataSourceException {
		Set<CrawlJob> returnValue = new HashSet<>(outstandingJobs);
		return returnValue;
	}

	@Override
	public Set<CrawlJob> finishCrawlingRobotsFile(CrawlJob job, Set<RobotsRule> newRules) throws DataSourceException {
		outstandingJobs.remove(job);
		receivedRules.put(job, newRules);
		outstandingJobs.removeAll(disallowedJobs.get(job));
		return disallowedJobs.get(job);
	}

	@Override
	public Set<CrawlJob> finishCrawlingHtmlFile(CrawlJob job, Set<URL> newUrls, String content) throws DataSourceException {
		outstandingJobs.remove(job);
		receivedUrls.put(job, newUrls);
		receivedContent.put(job, content);
		Set<CrawlJob> returnValue = new HashSet<CrawlJob>();
		for(CrawlJob newJob : newJobs.get(job)) {
			boolean isDisallowed = false;
			Iterator<CrawlJob> iter = receivedRules.keySet().iterator();
			while(!isDisallowed && iter.hasNext()) {
				CrawlJob robotsJob = iter.next();
				if(disallowedJobs.get(robotsJob).contains(newJob)) {
					isDisallowed = true;
				}
			}
			if(!isDisallowed) {
				outstandingJobs.add(newJob);
				returnValue.add(newJob);
			}
		}
		return returnValue;
	}
	
	@Override
	public void cancelCrawlingHtmlFile(CrawlJob job) throws DataSourceException {
		receivedCancellations.add(job);
		outstandingJobs.remove(job);
	}

	/**
	 * Checks that the results of a run through the CrawlController were correct.
	 */
	public void checkResults() {
		assertTrue("The following jobs should have been finished but were not: " + outstandingJobs + ".", outstandingJobs.isEmpty());
		assertEquals("The set of rules found in robots.txt files was not correct.", expectedRules, receivedRules);
		assertEquals("The set of URLs found in links in HTML files was not correct.", expectedUrls, receivedUrls);
		assertEquals("The set of finished HTML files was not correct.", expectedContent.keySet(), receivedContent.keySet());
		for(CrawlJob job : expectedContent.keySet()) {
			String expected = expectedContent.get(job).trim().replaceAll("\\s+", " ");
			String received = expectedContent.get(job).trim().replaceAll("\\s+", " ");
			assertEquals("The contents of " + job + " were not correct.", expected, received);
		}
		assertEquals("The set of cancelled HTML files was not correct.", expectedCancellations, receivedCancellations);
	}
}